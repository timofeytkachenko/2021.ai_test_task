{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from typing import Any, Dict, Literal, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sidetable as stb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import (PrecisionRecallDisplay, RocCurveDisplay,\n",
    "                             classification_report, confusion_matrix, f1_score,\n",
    "                             precision_recall_curve, recall_score)\n",
    "from sklearn.model_selection import (RepeatedStratifiedKFold, StratifiedKFold,\n",
    "                                     train_test_split)\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_outliers_iqr_multi(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove outliers from multiple columns in a DataFrame using the IQR method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame.\n",
    "    cols : list of str\n",
    "        List of column names to clean.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with outliers removed.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> df_filtered = remove_outliers_iqr_multi(df, ['col1', 'col2'])\n",
    "    \"\"\"\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    for col in cols:\n",
    "        q1, q3 = df[col].quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "        mask &= df[col].between(lower, upper)\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "def add_engineered_features(df: pd.DataFrame, inplace: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add engineered features for GBDT models to a copy of the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Original Bank Marketing DataFrame.\n",
    "    inplace : bool, optional\n",
    "        If True, modify the input df in-place. If False (default), return modified copy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added features.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    # === Age binning ===\n",
    "    df[\"age_group\"] = pd.cut(\n",
    "        df[\"age\"],\n",
    "        bins=[0, 30, 55, np.inf],  # cover all realistic ages\n",
    "        labels=[\"young\", \"middle\", \"senior\"],\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    # === Contact intensity ===\n",
    "    df[\"contact_intensity\"] = df[\"contacts_per_campaign\"] / (\n",
    "        df[\"nb_previous_contact\"] + 1\n",
    "    )\n",
    "\n",
    "    # === Education × Marital cross-feature ===\n",
    "    df[\"education_marital\"] = (\n",
    "        df[\"education\"].astype(str) + \"_\" + df[\"marital_status\"].astype(str)\n",
    "    )\n",
    "\n",
    "    # === Was previously contacted ===\n",
    "    df[\"was_previously_contacted\"] = (df[\"nb_previous_contact\"] > 0).astype(int)\n",
    "\n",
    "    # === Long contact binary flag (more than 5 minutes) ===\n",
    "    df[\"long_last_contact\"] = (df[\"last_contact_duration\"] > 300).astype(int)\n",
    "\n",
    "    # === Recent contact flag (last 30 days) ===\n",
    "    df[\"recently_contacted\"] = (df[\"N_last_days\"] < 30).astype(int)\n",
    "\n",
    "    # === N_last_days == 999 → new binary flag + cleaned version ===\n",
    "    df[\"never_contacted_before\"] = (df[\"N_last_days\"] == 999).astype(int)\n",
    "    df[\"N_last_days_cleaned\"] = df[\"N_last_days\"].replace(999, np.nan)\n",
    "\n",
    "    # === Economic stress proxy ===\n",
    "    df[\"economic_pressure\"] = -df[\"cons_conf_index\"] * df[\"emp_var_rate\"]\n",
    "\n",
    "    # === Interest spread: CPI - Euribor ===\n",
    "    df[\"interest_diff\"] = df[\"cons_price_index\"] - df[\"euri_3_month\"]\n",
    "\n",
    "    # === Month + weekday combined ===\n",
    "    df[\"month_weekday\"] = df[\"month\"].astype(str) + \"_\" + df[\"week_day\"].astype(str)\n",
    "\n",
    "    # === Contact pressure: calls / (days since last + 1) ===\n",
    "    df[\"contact_pressure\"] = df[\"contacts_per_campaign\"] / (\n",
    "        df[\"N_last_days_cleaned\"] + 1\n",
    "    )\n",
    "\n",
    "    # === Log features (avoid skew) ===\n",
    "    df[\"log_duration\"] = np.log1p(df[\"last_contact_duration\"])\n",
    "    df[\"log_contacts\"] = np.log1p(df[\"contacts_per_campaign\"])\n",
    "    df[\"log_previous\"] = np.log1p(df[\"nb_previous_contact\"])\n",
    "\n",
    "    # === Month → Season mapping ===\n",
    "    month_to_season = {\n",
    "        \"mar\": \"spring\",\n",
    "        \"apr\": \"spring\",\n",
    "        \"may\": \"spring\",\n",
    "        \"jun\": \"summer\",\n",
    "        \"jul\": \"summer\",\n",
    "        \"aug\": \"summer\",\n",
    "        \"sep\": \"autumn\",\n",
    "        \"oct\": \"autumn\",\n",
    "        \"nov\": \"autumn\",\n",
    "        \"dec\": \"winter\",\n",
    "        \"jan\": \"winter\",\n",
    "        \"feb\": \"winter\",\n",
    "    }\n",
    "    df[\"season\"] = df[\"month\"].map(month_to_season)\n",
    "\n",
    "    # === Previous outcome was success ===\n",
    "    df[\"prev_outcome_success\"] = (df[\"previous_outcome\"] == \"success\").astype(int)\n",
    "\n",
    "    # === Marketing intensity per employee ===\n",
    "    df[\"contact_to_employee_ratio\"] = df[\"contacts_per_campaign\"] / (\n",
    "        df[\"nb_employees\"] + 1\n",
    "    )\n",
    "\n",
    "    # === Stress per employee ===\n",
    "    df[\"stress_per_employee\"] = df[\"economic_pressure\"] / (df[\"nb_employees\"] + 1)\n",
    "\n",
    "    # === Age per contact — a proxy for target segment targeting ===\n",
    "    df[\"age_per_contact\"] = df[\"age\"] / (df[\"contacts_per_campaign\"] + 1)\n",
    "\n",
    "    # === Both loans flag ===\n",
    "    df[\"both_loans\"] = (\n",
    "        (df[\"housing_loan\"] == \"yes\") & (df[\"personal_loan\"] == \"yes\")\n",
    "    ).astype(int)\n",
    "\n",
    "    # === Loan burden score ===\n",
    "    df[\"loan_burden_score\"] = (df[\"housing_loan\"] == \"yes\").astype(int) + (\n",
    "        df[\"personal_loan\"] == \"yes\"\n",
    "    ).astype(int)\n",
    "\n",
    "    # === Risky contact (short but many calls) ===\n",
    "    df[\"risky_contact\"] = (\n",
    "        (df[\"last_contact_duration\"] < 90) & (df[\"contacts_per_campaign\"] > 3)\n",
    "    ).astype(int)\n",
    "\n",
    "    # === Employment × Confidence — interaction signal ===\n",
    "    df[\"emp_conf_product\"] = df[\"emp_var_rate\"] * df[\"cons_conf_index\"]\n",
    "\n",
    "    # === Volatility index ===\n",
    "    df[\"volatility_score\"] = (\n",
    "        df[\"emp_var_rate\"].abs()\n",
    "        + df[\"cons_conf_index\"].abs()\n",
    "        + df[\"euri_3_month\"].abs()\n",
    "    )\n",
    "\n",
    "    # === Recent effective contact score ===\n",
    "    df[\"recent_contact_score\"] = df[\"recently_contacted\"] * df[\"long_last_contact\"]\n",
    "\n",
    "    df = df.drop(columns=[\"N_last_days_cleaned\"])\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[\"N_last_days\"] = df[\"N_last_days\"].astype(\"category\")\n",
    "\n",
    "    # 1. Add new category\n",
    "    df[\"N_last_days\"] = df[\"N_last_days\"].cat.add_categories(\"no previous contacts\")\n",
    "\n",
    "    # 2. Replace 999 with 'no previous contacts'\n",
    "    df.loc[df[\"N_last_days\"] == 999, \"N_last_days\"] = \"no previous contacts\"\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_data(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    val_size: float = 0.1,\n",
    "    test_size: float = 0.1,\n",
    "    random_state: int = 42,\n",
    "    use_stratification: bool = False,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Split input data into train, validation, and test sets, optionally using stratification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix.\n",
    "    y : pd.Series\n",
    "        Target variable.\n",
    "    val_size : float, optional\n",
    "        Proportion of the data to include in the validation set, by default 0.1.\n",
    "    test_size : float, optional\n",
    "        Proportion of the data to include in the test set, by default 0.1.\n",
    "    random_state : int, optional\n",
    "        Random seed for reproducible splits, by default 42.\n",
    "    use_stratification : bool, optional\n",
    "        If True, data splits will be stratified using the target variable. By default False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]\n",
    "        A tuple containing:\n",
    "        - X_train: pd.DataFrame\n",
    "            Training feature matrix\n",
    "        - X_val: pd.DataFrame\n",
    "            Validation feature matrix\n",
    "        - X_test: pd.DataFrame\n",
    "            Test feature matrix\n",
    "        - y_train: pd.Series\n",
    "            Training target vector\n",
    "        - y_val: pd.Series\n",
    "            Validation target vector\n",
    "        - y_test: pd.Series\n",
    "            Test target vector\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the sum of val_size and test_size is greater than or equal to 1.\n",
    "    \"\"\"\n",
    "    # Check that the requested splits are valid\n",
    "    if val_size + test_size >= 1:\n",
    "        raise ValueError(\"val_size + test_size must be less than 1.\")\n",
    "\n",
    "    # Determine stratification parameters\n",
    "    stratify_test = y if use_stratification else None\n",
    "\n",
    "    # First split off the test set\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=stratify_test,  # stratify if specified\n",
    "    )\n",
    "\n",
    "    # Adjust val_size proportion relative to the remaining data after removing test\n",
    "    adjusted_val_size = val_size / (1 - test_size)\n",
    "\n",
    "    # For the second split (train/val), stratify on the reduced dataset if needed\n",
    "    stratify_val = y_temp if use_stratification else None\n",
    "\n",
    "    # Split the remaining data into train and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=adjusted_val_size,\n",
    "        random_state=random_state,\n",
    "        stratify=stratify_val,  # stratify if specified\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ],
   "id": "ea4323afe9706aa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"bank_dataset (3) (1) (1) (3) (1) (2) (1) (1) (1).csv\")"
   ],
   "id": "cd9d3521560b4c74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.head()"
   ],
   "id": "7d0014afaa54750d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### EDA",
   "id": "9da622d51eef639e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.info()"
   ],
   "id": "41b8dc4df688f9fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.stb.missing(style=True)"
   ],
   "id": "84aaa2164702eb4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.stb.freq([\"target\"], style=True)"
   ],
   "id": "2390cfcfb36254bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.stb.freq([\"occupation\", \"marital_status\"], style=True)"
   ],
   "id": "2a7e1c1d9cc5728f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.stb.freq([\"education\"], style=True)"
   ],
   "id": "a33d889c58858dda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.stb.counts()"
   ],
   "id": "d16e491ef04433d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    sns.countplot(x=col, data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "eb76309e9e24f591",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for col in df.select_dtypes(include=[\"int64\", \"float64\"]).columns:\n",
    "    sns.kdeplot(x=df[col], fill=True)\n",
    "    plt.show()"
   ],
   "id": "ab9817781cc47a6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.pairplot(\n",
    "    df[df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist() + [\"target\"]].sample(n=5000, random_state=42),\n",
    "    hue=\"target\",\n",
    "    diag_kind=\"kde\",\n",
    ")"
   ],
   "id": "f0fdf9f04701e082",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# N_last_days = 999 means that the customer has no previous contacts\n",
    "df[df[\"N_last_days\"] == 999].stb.counts()"
   ],
   "id": "44fa07ec8114c994",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add new features\n",
    "df = add_engineered_features(df)"
   ],
   "id": "2846293b31d14ab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Final dataframe",
   "id": "b74dbb53ae2590ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "432c42e1413df31d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.stb.counts()",
   "id": "257769bfde685807",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Modeling",
   "id": "ce113486187cd30c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categorial_features = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "categorial_features"
   ],
   "id": "a8d63a77794e07d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categorial_features = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "categorial_features.remove(\"target\")\n",
    "categorial_features.extend([\"age_group\", \"education_marital\", \"month_weekday\", \"season\", \"was_previously_contacted\",\n",
    "                            \"long_last_contact\", \"recently_contacted\", \"never_contacted_before\", \"prev_outcome_success\",\n",
    "                            \"both_loans\", \"loan_burden_score\", \"risky_contact\", \"recent_contact_score\"])\n",
    "categorial_features"
   ],
   "id": "225798672a7151db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_dict = {\"yes\": 1, \"no\": 0}\n",
    "df.loc[:, \"target\"] = df[\"target\"].map(target_dict)\n",
    "df.loc[:, \"target\"] = df.astype(\"category\")"
   ],
   "id": "a9cd03c210519eb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y = df.pop(\"target\")\n",
    "y = y.astype(int)"
   ],
   "id": "2815a9fffdce5ae8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.head()"
   ],
   "id": "4d7026e981f318f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "    df, y, val_size=0.1, test_size=0.2, use_stratification=True, random_state=42\n",
    ")"
   ],
   "id": "5d70d889d6da2e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_full = pd.concat([X_train, X_val], axis=0)\n",
    "y_train_full = pd.concat([y_train, y_val], axis=0)"
   ],
   "id": "cf48b62041793646",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_val.value_counts(normalize=True)"
   ],
   "id": "2a8b3e181b1b89dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_train_full.value_counts(normalize=True)"
   ],
   "id": "f294e7c863a534a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_train_full.info()"
   ],
   "id": "13c38f4bf241efde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "classes = np.unique(y_train_full)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=classes, y=y_train_full\n",
    ")\n",
    "class_weights = class_weights.tolist()\n",
    "class_weights"
   ],
   "id": "7fcf1e9202351b97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def optimize_catboost(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    n_splits: int = 5,\n",
    "    n_repeats: Optional[int] = None,\n",
    "    n_trials: int = 50,\n",
    "    random_state: int = 42,\n",
    "    max_iterations: int = 1000,\n",
    "    optimize_metric: Literal[\"F1\", \"Recall\"] = \"F1\",\n",
    ") -> Tuple[optuna.Study, Dict[str, Any], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Optimize CatBoost hyperparameters for imbalanced classification, including class_weights,\n",
    "    using StratifiedKFold/RepeatedStratifiedKFold and Optuna.\n",
    "    \"\"\"\n",
    "\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    if optimize_metric not in [\"F1\", \"Recall\"]:\n",
    "        raise ValueError(\"optimize_metric must be either 'F1' or 'Recall'.\")\n",
    "\n",
    "    assert not y.isnull().any(), \"Target y contains NaN values.\"\n",
    "\n",
    "    def objective(trial: optuna.Trial) -> float:\n",
    "        \"\"\"Objective function to optimize.\"\"\"\n",
    "        params = {\n",
    "            \"loss_function\": \"Logloss\",\n",
    "            \"eval_metric\": optimize_metric,\n",
    "            \"custom_metric\": [\"Logloss\"],\n",
    "            \"iterations\": max_iterations,\n",
    "            \"use_best_model\": False,\n",
    "            \"metric_period\": 1,\n",
    "            \"depth\": trial.suggest_int(\"depth\", 3, 14),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.4, log=True),\n",
    "            \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 1, 100, log=True),\n",
    "            \"random_strength\": trial.suggest_float(\n",
    "                \"random_strength\", 1e-2, 10.0, log=True\n",
    "            ),\n",
    "            \"border_count\": trial.suggest_int(\"border_count\", 32, 256),\n",
    "            \"auto_class_weights\": \"Balanced\",\n",
    "            \"verbose\": 0,\n",
    "            \"random_seed\": random_state,\n",
    "            \"allow_writing_files\": False,\n",
    "        }\n",
    "\n",
    "        if n_repeats:\n",
    "            skf = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    "            )\n",
    "        else:\n",
    "            skf = StratifiedKFold(\n",
    "                n_splits=n_splits, shuffle=True, random_state=random_state\n",
    "            )\n",
    "\n",
    "        scoring_func = recall_score if optimize_metric == \"Recall\" else f1_score\n",
    "\n",
    "        fold_scores = []\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            model = CatBoostClassifier(**params, cat_features=categorial_features)\n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "\n",
    "            y_pred = model.predict(X_val)\n",
    "            fold_scores.append(scoring_func(y_val, y_pred, pos_label=1))\n",
    "\n",
    "        return np.mean(fold_scores)\n",
    "\n",
    "    # Create and configure Optuna study\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "\n",
    "    # Merge static params back\n",
    "    fixed_params = {\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": optimize_metric,\n",
    "        \"custom_metric\": [\"Logloss\"],\n",
    "        \"iterations\": max_iterations,\n",
    "        \"use_best_model\": False,\n",
    "        \"metric_period\": 1,\n",
    "        \"verbose\": 0,\n",
    "        \"random_seed\": random_state,\n",
    "        \"allow_writing_files\": False,\n",
    "    }\n",
    "\n",
    "    best_params.update(fixed_params)\n",
    "\n",
    "    # Final CV pass\n",
    "    def cross_val_iterations(\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        params: Dict[str, Any],\n",
    "        n_splits: int,\n",
    "        n_repeats: Optional[int],\n",
    "        random_state: int,\n",
    "        main_metric: str,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Manual CV to gather iteration-wise statistics.\"\"\"\n",
    "        if n_repeats:\n",
    "            skf = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    "            )\n",
    "        else:\n",
    "            skf = StratifiedKFold(\n",
    "                n_splits=n_splits, shuffle=True, random_state=random_state\n",
    "            )\n",
    "\n",
    "        train_metric_folds, test_metric_folds = [], []\n",
    "        train_logloss_folds, test_logloss_folds = [], []\n",
    "\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            model = CatBoostClassifier(**params, cat_features=categorial_features)\n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "\n",
    "            results = model.get_evals_result()\n",
    "            train_key, test_key = \"learn\", \"validation\"\n",
    "\n",
    "            train_metric_folds.append(results[train_key][main_metric])\n",
    "            test_metric_folds.append(results[test_key][main_metric])\n",
    "            train_logloss_folds.append(\n",
    "                results[train_key].get(\"Logloss\", results[train_key][\"TotalLoss\"])\n",
    "            )\n",
    "            test_logloss_folds.append(\n",
    "                results[test_key].get(\"Logloss\", results[test_key][\"TotalLoss\"])\n",
    "            )\n",
    "\n",
    "        n_iters = len(train_metric_folds[0])\n",
    "\n",
    "        data = []\n",
    "        for i in range(n_iters):\n",
    "            data.append(\n",
    "                {\n",
    "                    \"iteration\": i + 1,\n",
    "                    f\"train-{main_metric}-mean\": np.mean(\n",
    "                        [fold[i] for fold in train_metric_folds]\n",
    "                    ),\n",
    "                    f\"train-{main_metric}-std\": np.std(\n",
    "                        [fold[i] for fold in train_metric_folds], ddof=1\n",
    "                    ),\n",
    "                    f\"test-{main_metric}-mean\": np.mean(\n",
    "                        [fold[i] for fold in test_metric_folds]\n",
    "                    ),\n",
    "                    f\"test-{main_metric}-std\": np.std(\n",
    "                        [fold[i] for fold in test_metric_folds], ddof=1\n",
    "                    ),\n",
    "                    \"train-Logloss-mean\": np.mean(\n",
    "                        [fold[i] for fold in train_logloss_folds]\n",
    "                    ),\n",
    "                    \"train-Logloss-std\": np.std(\n",
    "                        [fold[i] for fold in train_logloss_folds], ddof=1\n",
    "                    ),\n",
    "                    \"test-Logloss-mean\": np.mean(\n",
    "                        [fold[i] for fold in test_logloss_folds]\n",
    "                    ),\n",
    "                    \"test-Logloss-std\": np.std(\n",
    "                        [fold[i] for fold in test_logloss_folds], ddof=1\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    cv_results = cross_val_iterations(\n",
    "        X, y, best_params, n_splits, n_repeats, random_state, optimize_metric\n",
    "    )\n",
    "\n",
    "    return study, best_params, cv_results"
   ],
   "id": "d183049649c5cdb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optimize and retrieve iteration-wise CV results\n",
    "study_obj, best_params, cv_table = optimize_catboost(\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    optimize_metric=\"F1\",\n",
    "    n_splits=3,\n",
    "    n_trials=20,\n",
    "    random_state=42,\n",
    "    max_iterations=2000\n",
    ")"
   ],
   "id": "84d0fb238134517f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print results\n",
    "print(\"Best F1:\", study_obj.best_value)\n",
    "\n",
    "best_params[\"iterations\"] = cv_table[\"test-F1-mean\"].idxmax()\n",
    "print(\"Best parameters:\", best_params)"
   ],
   "id": "f3f8a47bd2fac969",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = CatBoostClassifier(\n",
    "    cat_features=categorial_features,\n",
    "    loss_function=best_params[\"loss_function\"],\n",
    "    eval_metric=best_params[\"eval_metric\"],\n",
    "    iterations=best_params[\"iterations\"],\n",
    "    depth=best_params[\"depth\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    l2_leaf_reg=best_params[\"l2_leaf_reg\"],\n",
    "    random_strength=best_params[\"random_strength\"],\n",
    "    border_count=best_params[\"border_count\"],\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    random_seed=42,\n",
    ")"
   ],
   "id": "41edae8d90abf041",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.fit(X_train_full, y_train_full)"
   ],
   "id": "2a17742041e00f40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pred = model.predict(X_test)\n",
    "pred_proba = model.predict_proba(X_test)[:, 1]"
   ],
   "id": "20160e387fd0d92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(classification_report(y_test, pred))"
   ],
   "id": "7c54f66bcec0e8ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "confusion_matrix(y_test, pred)"
   ],
   "id": "c5f79f3a894b92a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_test,\n",
    "    pred_proba,\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True,\n",
    ")\n",
    "_ = display.ax_.set(xlabel=\"False Positive Rate\", ylabel=\"True Positive Rate\")"
   ],
   "id": "9c2d66994a742908",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    model, X_test, y_test, name=\"Pipeline\", plot_chance_level=True\n",
    ")\n",
    "_ = display.ax_.set_title(\"2-class Precision-Recall curve\")"
   ],
   "id": "5faa0d6c17e0251e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, pred_proba)\n",
    "thresholds = np.append(thresholds, 1.0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(thresholds, precision, label=\"Precision\", color=\"blue\")\n",
    "plt.plot(thresholds, recall, label=\"Recall\", color=\"green\")\n",
    "\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Precision and Recall vs Thresholds\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "91d915b16096c320",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d4f8e60ca45b4cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1bd6515e3ccafccd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
